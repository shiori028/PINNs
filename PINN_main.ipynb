{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[],"collapsed_sections":["q-fhSHfJVa-g"],"machine_shape":"hm"},"kernelspec":{"display_name":"Python 3","name":"python3"},"language_info":{"name":"python"},"accelerator":"GPU"},"cells":[{"cell_type":"markdown","metadata":{"id":"q-fhSHfJVa-g"},"source":["# Import libraries"]},{"cell_type":"code","metadata":{"id":"GwQXM3jdU9VD"},"source":["import torch\n","import torch.nn as nn\n","from torch.autograd import Variable\n","from torch.autograd import grad\n","import numpy as np\n","from copy import deepcopy\n","import pickle\n","import matplotlib.pyplot as plt\n","device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n","from natsort import natsorted"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"1nGHLECrVYvA","executionInfo":{"status":"ok","timestamp":1646139970313,"user_tz":-540,"elapsed":10,"user":{"displayName":"Shiori Kubo","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"12860308473170441456"}},"outputId":"a2781638-b838-483f-cc12-480fbc94492c"},"source":["device"],"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["device(type='cuda', index=0)"]},"metadata":{},"execution_count":309}]},{"cell_type":"markdown","metadata":{"id":"SDwrZLWiVeTQ"},"source":["# Model definition"]},{"cell_type":"code","metadata":{"id":"Um4mJzXoVYsU"},"source":["class SurrogateModel(nn.Module):\n","  def __init__(self, name=None):        \n","    super().__init__()\n","    \n","    # NN module\n","    self.stack = nn.Sequential(\n","        nn.Linear(2, 20),\n","        nn.Tanh(),\n","        nn.Linear(20,20),\n","        nn.Tanh(),\n","        nn.Linear(20,20),\n","        nn.Tanh(),\n","        nn.Linear(20,20),\n","        nn.Tanh(),\n","        nn.Linear(20,20),\n","        nn.Tanh(),\n","        nn.Linear(20,20),\n","        nn.Tanh(),\n","        nn.Linear(20,20),\n","        nn.ReLU(),\n","        #nn.Tanh(),\n","        nn.Linear(20,20),\n","        nn.ReLU(),\n","        #nn.Tanh(),\n","        nn.Linear(20,20),\n","        nn.ReLU(),\n","        #nn.Tanh(),\n","        nn.Linear(20,1)\n","    )\n","\n","  # Forward calculation\n","  def forward(self, x):\n","    return self.stack(x)"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"hqUxyYGDVi6I"},"source":["## Physics : Terzaghi's equation "]},{"cell_type":"code","metadata":{"id":"jCSBdhJBVYm6"},"source":["class Consolidation_Equation():\n","  def __init__(self, model, z_obs, t_obs, z_phy, t_phy, upper, lower, cv):\n","    self.model = model\n","    self.z_obs = z_obs\n","    self.t_obs = t_obs\n","    self.z_phy = z_phy\n","    self.t_phy = t_phy\n","    self.cv = cv\n","\n","    self.z_max = upper[0] \n","    self.z_min = lower[0] \n","    self.t_max = upper[1] \n","    self.t_min = lower[1] \n","    self.e_max = upper[2]\n","    self.e_min = lower[2]\n","\n","    self.z_sf = self.z_max - self.z_min \n","    self.t_sf = self.t_max - self.t_min \n","    self.e_sf = self.e_max - self.e_min \n","\n","  # Prediction\n","  def predict(self, z, t):\n","\n","    # Store input into a tensor\n","    z_sd = (z - self.z_min)/self.z_sf\n","    t_sd = (t - self.t_min)/self.t_sf\n","    input = torch.cat((z_sd, t_sd), 1)\n","\n","    e = self.e_sf * self.model(input)\n","\n","    return e\n","\n","  # Governing Equation\n","  def GE(self, z, t, e, cv):\n","\n","    e_t = self.t_sf * grad(e.sum(), t, create_graph=True)[0] \n","    e_z = self.z_sf * grad(e.sum(), z, create_graph=True)[0]\n","\n","    e_zz = self.z_sf * grad(e_z.sum(), z, create_graph=True)[0] \n","    \n","\n","    # Governing Equation : Darcy's law\n","    f_e = e_t - self.t_sf/self.z_sf/self.z_sf * self.cv * e_zz \n","\n","    return f_e\n","    \n","  def train(self, N_Iter, N_out, optimizer, weight=0.5, eps=1e-10):\n","\n","    N_obs = self.z_obs.size()[0]\n","    N_phy = self.z_phy.size()[0]\n","\n","    for iter in range(N_Iter):\n","\n","      if N_phy > 0:\n","        # Observation + Physics\n","        # Predictions\n","        e_pre = self.predict(self.z_obs, self.t_obs)\n","        \n","        # Governing Equations\n","        e_test = self.predict(self.z_phy, self.t_phy)         \n","        f_e = self.GE(self.z_phy, self.t_phy, e_test, self.cv) \n","\n","        # Loss functions\n","        # vs. observation\n","        loss_obs = (torch.square(e_pre - e_obs).sum())/N_obs\n","\n","        # vs. physics\n","        loss_phy = (torch.square(f_e).sum())/N_phy\n","\n","        # Total error\n","        loss = weight*loss_obs + (1.0-weight)*loss_phy\n","\n","        \n","      else:\n","        # Observation only\n","        # Predictions\n","        e_pre = self.predict(self.z_obs, self.t_obs)\n","\n","        # Loss functions\n","        # only vs. observation\n","        loss = (torch.square(e_pre - e_obs).sum())/N_obs\n","\n","      # Backpropagation\n","      optimizer.zero_grad()\n","      loss.backward()\n","      optimizer.step()\n","        \n","      if loss.item() < eps:\n","        print('break')\n","        break\n","\n","      if iter % N_out == 0:\n","        print(f\"Loss: {loss.item():>7f}  [{iter:>5d}/{N_Iter:>5d}]\")\n","\n","    print(f\"Loss: {loss.item():>7f}  [{N_Iter:>5d}/{N_Iter:>5d}]\")\n","    if N_phy > 0:\n","      print(f\" obs : {loss_obs.item():>5e}\")\n","      print(f\" phy : {loss_phy.item():>5e}\")\n","    print(f\" cv : {self.cv.item():>5e}\")\n","\n","    return loss.item(), loss_obs.item(), loss_phy.item()\n","\n","  def test(self, z_test, t_test, e_test):\n"," \n","    N_test = z_test.size()[0]\n","    \n","    e_pre = self.predict(z_test, t_test)\n","    error = (torch.square(e_pre - e_test).sum() )/N_test\n","\n","    print(f\"Test Error: {error.item():>7f}\")\n","\n","    return error.item()"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"tEavOI97VniD"},"source":["# Load simulation data"]},{"cell_type":"markdown","metadata":{"id":"oqU5xdzCVpjy"},"source":["## Mount Google Drive in Google Colab"]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"4LJb-Tb9VYj7","executionInfo":{"status":"ok","timestamp":1646139972713,"user_tz":-540,"elapsed":2407,"user":{"displayName":"Shiori Kubo","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"12860308473170441456"}},"outputId":"695912cf-9cd8-4cb6-e7b0-a536cd3ce776"},"source":["from google.colab import drive\n","drive.mount('/content/drive')"],"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"]}]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"Um54Tdp9VYhB","executionInfo":{"status":"ok","timestamp":1646139972714,"user_tz":-540,"elapsed":10,"user":{"displayName":"Shiori Kubo","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"12860308473170441456"}},"outputId":"0d855993-7d89-43de-9e0c-3cda41811177"},"source":["import os\n","os.chdir('/content/drive')\n","print(os.getcwd())"],"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["/content/drive/MyDrive/20210810_PINN/20211208/e/data/PINN\n"]}]},{"cell_type":"markdown","metadata":{"id":"zYnCE8HRVwro"},"source":["## Load data"]},{"cell_type":"code","metadata":{"id":"MelHRQWwVYbq"},"source":["import scipy.io\n","import glob\n","import os\n","from scipy.interpolate import griddata\n","\n","files_obs = glob.glob('10days/*')\n","files_obs = natsorted(files_obs)\n","\n","data_obs = np.empty((0,2))\n","\n","for file in files_obs:\n","  filename = os.path.basename(file)\n","  temp_obs = np.loadtxt('10days/' + filename, ndmin=2, delimiter=\" \")\n","  data_obs = np.append(data_obs, temp_obs, axis=0)\n","\n","# Observation data\n","t_org = np.arange(len(files_obs)).reshape(len(files_obs),1) * 3600\n","\n","ZZ = data_obs[:,0].reshape((-1, len(files_obs)))\n","EE = data_obs[:,1].reshape((-1, len(files_obs)))\n","\n","N = ZZ.shape[0]\n","T = t_org.shape[0]\n","\n","TT = np.tile(t_org, (1,N)).T\n","\n","z = ZZ.flatten()[:,None] # NT x 1\n","ttt = TT.flatten()[None,:] # NT x 1\n","t = (np.sort(ttt)).T\n","e = EE.flatten()[:,None] # NT x 1\n","\n","# Physics data\n","t_p1 = np.arange(0.,8640001., 3600.).reshape(-1,1)\n","t_p = np.repeat(t_p1,501).reshape(-1,1).flatten()[:,None] \n","z_p1 = np.arange(0.,5.01,0.01).reshape(-1,1)\n","z_p = []\n","for i in range(2401):\n","  z_p.append(z_p1)\n","z_p = np.array(z_p).reshape(-1,1).flatten()[:,None] \n","\n","N_p = z_p1.shape[0]\n","T_p = t_p1.shape[0]\n","\n","# Expected upper and lower bounds of input for surrogate model\n","upper = np.array([z_p.max(), t_p.max(), e_o.max()])\n","lower = np.array([z_p.min(), t_p.min(), e_o.min()])\n","\n","true = np.loadtxt('true/e_8640000.txt', delimiter = \" \", dtype = 'float32')\n","t_true = np.full((501,1), 8640000., dtype = 'float32')\n","z_true = true[:,0].flatten()[:,None]\n","e_true = true[:,1].flatten()[:,None]"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"DtZ0mXfSVzyg"},"source":["# Observation and Test dataset (N_obs = 10,000, N_test = 501)"]},{"cell_type":"code","metadata":{"id":"05bvJUOtVYY4"},"source":["# Number of observation and test points\n","N_obs = 10000\n","N_test = 501\n","\n","np.random.seed(123453)\n","\n","# Index list for observation and validation data\n","idx_obs = np.random.choice(N*T, N_obs, replace=False)\n","\n","# Observation points for training\n","z_obs = Variable(torch.tensor(z[idx_obs,:].astype(np.float32)), requires_grad=True).to(device)\n","t_obs = Variable(torch.tensor(t[idx_obs,:].astype(np.float32)), requires_grad=True).to(device)\n","\n","# Observed velocity field for training\n","e_obs = torch.tensor(e[idx_obs,:].astype(np.float32)).to(device)\n","\n","# Collocation points for validation\n","z_test = torch.tensor(z_true).to(device) \n","t_test = torch.tensor(t_true).to(device) \n","\n","# Observed velocity field for validation\n","e_test = torch.tensor(e_true).to(device)"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"QCpcdWc-WW9s"},"source":["# Train using observation and physics (N_phy = N_obs = 10,000, phy =! obs)"]},{"cell_type":"code","metadata":{"id":"-ynTN5jNV3TA"},"source":["# Model input and parameters\n","N_phy = 10000 \n","\n","np.random.seed(123453)\n","\n","# Index list for observation and validation data\n","idx_phy = np.random.choice(N_p*T_p, N_phy, replace=False)\n","\n","# Points for training\n","z_phy = Variable(torch.tensor(z_p[idx_phy,:].astype(np.float32)), requires_grad=True).to(device)\n","t_phy = Variable(torch.tensor(t_p[idx_phy,:].astype(np.float32)), requires_grad=True).to(device)\n","\n","# Collocation points for physics\n","#z_phy = z_obs\n","#t_phy = t_obs"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"HUzMXOOOgPYC"},"source":["##Noise = 0.0"]},{"cell_type":"markdown","metadata":{"id":"YwwB6cMVWdyU"},"source":["### Allocation of variables and model parameters"]},{"cell_type":"code","metadata":{"id":"xbLCjG4GV3Qi"},"source":["# Model input and parameters\n","\n","noise_level = 0.0\n","\n","# Observed velocity field for training\n","e_obs = torch.tensor(e[idx_obs,:].astype(np.float32)).to(device)\n","\n","# Additional noise\n","e_noise = torch.tensor(noise_level * (e.max()-e.min()) * np.random.randn(N_obs,1)).to(device)\n","\n","e_obs = e_obs + e_noise\n","\n","# Model parameters in governing equation\n","cv = torch.tensor(5.2 * pow(10, -7)).to(device)\n","\n","# tensors to normalize input for NN surrogate model\n","ub = torch.tensor(upper.astype(np.float32)).to(device)\n","lb = torch.tensor(lower.astype(np.float32)).to(device)"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"aMPWaFmrWg2B"},"source":["### No.1: Training loop for model parameters in surrogate model"]},{"cell_type":"code","metadata":{"id":"TpaXC7QQV3OB"},"source":["N_epoch = 40\n","N_Iter = 1000\n","N_out = 200\n","\n","# Model definition\n","torch.manual_seed(123453)\n","np.random.seed(123453)\n","\n","surrogate = SurrogateModel().to(device)\n","physics = Consolidation_Equation(surrogate,\n","                                 z_obs, t_obs,\n","                                 z_phy, t_phy,\n","                                 upper, lower,\n","                                 cv)\n","\n","\n","optimizer = torch.optim.Adam([\n","                              {'params': surrogate.parameters()},\n","                             ], lr=1e-3)\n","\n","train_loss = np.array([])\n","test_error = np.array([])\n","\n","loss_obs = np.array([])\n","loss_phy = np.array([])\n","\n","min_error = 1e15\n","\n","for epoch in range(N_epoch):\n","  print(f\"Epoch: {epoch:>5d}\")\n","  loss = physics.train(N_Iter, N_out, optimizer, eps=1e-15)\n","  error = physics.test(z_test, t_test, e_test)\n","  if error < min_error:\n","    min_error = error\n","    min_surrogate = deepcopy(surrogate)\n","  print(\"\")\n","  #print(loss)\n","\n","  train_loss = np.append(train_loss, loss[0])\n","  test_error = np.append(test_error, error)\n","\n","  loss_obs = np.append(loss_obs, loss[1])\n","  loss_phy = np.append(loss_phy, loss[2])\n","\n","  e_check = physics.predict(z_obs, t_obs)\n","  f_e = physics.GE(z_obs, t_obs, e_check, cv)\n","  e_predict = physics.predict(z_test, t_test)\n","  \n","  path = 'results/'\n","  np.savetxt(path + str(epoch) + 'e_pre.txt', e_check.to('cpu').detach())\n","  np.savetxt(path + str(epoch) + 'e_obs.txt', e_obs.to('cpu').detach())\n","  np.savetxt(path + str(epoch) + 'z_obs.txt', z_obs.to('cpu').detach())\n","  np.savetxt(path + str(epoch) + 't_obs.txt', t_obs.to('cpu').detach())\n","  np.savetxt(path + str(epoch) + 'f_e.txt', f_e.to('cpu').detach())\n","  np.savetxt(path + str(epoch) + 'e_test_true.txt', e_test.to('cpu').detach())\n","  np.savetxt(path + str(epoch) + 'e_test_pre.txt', e_predict.to('cpu').detach())  \n","  np.savetxt(path + str(epoch) + 'e_0_obs.txt', e_0_obs.to('cpu').detach())  "],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"Q-AaPoHpWkoh"},"source":["#### Minimum Estimation Error"]},{"cell_type":"code","source":["np.savetxt(path + 'train_loss.txt', train_loss) \n","np.savetxt(path + 'test_error.txt', test_error) \n","np.savetxt(path + 'loss_obs.txt', loss_obs) \n","np.savetxt(path + 'loss_phy.txt', loss_phy) \n","np.savetxt(path + 'loss_bc.txt', loss_bc) "],"metadata":{"id":"n8aJ03narwhU"},"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"N2N24jWuVYWQ"},"source":["epoch = np.arange(N_epoch)\n","fig = plt.figure(figsize=(10, 6))\n","ax1 = fig.add_subplot(111)\n","ax1.plot(epoch, train_loss, '-', color=red, label='train loss')\n","\n","\n","ax2 = ax1.twinx()\n","ax2.plot(epoch, test_error, '-', color=blue, label='test error')\n","\n","plt.xticks(fontsize=10)\n","plt.yticks(fontsize=10)\n","#plt.ylim(0, 4e-4)\n","ax1.set_xlabel('Epoch', fontsize=15)\n","ax1.set_ylabel('Loss', fontsize=15)\n","ax2.set_ylabel('Error', fontsize=15)\n","\n","ax1.ticklabel_format(axis=\"y\", scilimits=(0,0), useMathText=True)\n","ax2.ticklabel_format(axis=\"y\", scilimits=(0,0), useMathText=True)\n","\n","h1, l1 = ax1.get_legend_handles_labels()\n","h2, l2 = ax2.get_legend_handles_labels()\n","\n","#ax1.legend(h1+h2, l1+l2, loc='upper right', bbox_to_anchor=(0.18, 0.98), fontsize=10)\n","ax1.legend(h1+h2, l1+l2, loc='upper right', bbox_to_anchor=(0.98, 0.98), fontsize=10)\n","\n","#fig.subplots_adjust(left=0, right=1, bottom=0, top=1)\n","plt.savefig(path + 'loss_error_' + str(min_filenum_u) + 'epoch_' + str('{:.3e}'.format(test_error_u[min_filenum_u])) +  '.svg')\n","subprocess.call('inkscape sample.svg -M sample.emf',shell=True)\n","plt.show()"],"execution_count":null,"outputs":[]},{"cell_type":"code","source":["min_filenum_e = np.argmin(test_error)\n","e_min_loss = np.loadtxt(path + str(min_filenum_e) + 'e_test_pre.txt')\n","depth=np.arange(0.,5.01,0.01).reshape(-1,1)\n","\n","####################### strain #######################\n","fig = plt.figure(figsize=(6, 10))\n","ax = fig.add_subplot(111)\n","ax.plot(e_min_loss, depth, '-', color=black, label='Predicted strain') #, linestyle=\"dashed\")\n","#ax.plot(e_min_loss_uepoch, depth, '-', color=black, label='Predicted strain', linestyle=\"dashdot\")\n","ax.plot(e_true, depth, '-', color=gray, label='True')\n","plt.xticks(fontsize=10)\n","plt.yticks(fontsize=10)\n","\n","ax.set_xlabel('Strain', fontsize=15)\n","ax.set_ylabel('Depth (m)', fontsize=15)\n","\n","ax.invert_yaxis()\n","\n","ax.ticklabel_format(axis=\"y\", scilimits=(0,0), useMathText=True)\n","\n","#plt.legend(bbox_to_anchor=(0.42, 0.07), loc='upper right', borderaxespad=1, fontsize=10)\n","plt.legend(bbox_to_anchor=(0.43, 1.0), loc='upper right', borderaxespad=1, fontsize=10)\n","\n","plt.savefig(path + 'strain_' + str(min_filenum_e) + 'epoch_' + str('{:.6f}'.format(settlement_pre)) + '.svg')\n","#subprocess.call('inkscape sample.svg -M sample.emf',shell=True)\n","plt.show()"],"metadata":{"id":"WczkCZmVZzSS"},"execution_count":null,"outputs":[]}]}